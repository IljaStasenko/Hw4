{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506c261b-5a04-437a-9782-2600a221cbab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path('data/finetune_gpt/')\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "dataset = load_dataset(\"statworx/haiku\", cache_dir=DATA_PATH / 'model_cache')\n",
    "\n",
    "updated_data = [{'text': item['text'], 'keywords': item['keywords']} for item in dataset['train']]\n",
    "data_df = pd.DataFrame(updated_data)\n",
    "work_data = data_df.dropna(subset=['text', 'keywords'])\n",
    "work_data = work_data.drop_duplicates(subset=['text']).reset_index(drop=True)\n",
    "work_data['text'] = work_data['text'].str.replace(' / ', ' ')\n",
    "work_data = work_data[:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68460d7-e25c-4ac6-9319-b080f8c7cc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\–ò–ª—å—è\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from pathlib import Path\n",
    "\n",
    "class FineTuner:\n",
    "    def __init__(self, \n",
    "                 model_name='gpt2', \n",
    "                 cache_dir='model_cache',\n",
    "                 data_path=DATA_PATH):\n",
    "        self.data_path = Path(data_path)\n",
    "        \n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name, cache_dir=str(self.data_path / cache_dir))\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name, cache_dir=str(self.data_path / cache_dir))\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        \n",
    "        df['input'] = df.apply(lambda row: f\"{row['keywords']} {self.tokenizer.eos_token}\", axis=1)\n",
    "        \n",
    "        df['output'] = df.apply(lambda row: f\"{row['text']} {self.tokenizer.eos_token}\", axis=1)\n",
    "\n",
    "        print(df)\n",
    "        \n",
    "        dataset_path = self.data_path / 'train_dataset.txt'\n",
    "\n",
    "        with dataset_path.open('w', encoding='utf-8') as file:\n",
    "            for input_text, target_text in zip(df['input'], df['output']):\n",
    "                file.write(input_text + ' ' + target_text + '\\n')\n",
    "        return dataset_path\n",
    "\n",
    "    def fine_tune(self, \n",
    "                  dataset_path, \n",
    "                  output_name='fine_tuned_model', \n",
    "                  num_train_epochs=16, \n",
    "                  per_device_train_batch_size=4, \n",
    "                  learning_rate=5e-5, \n",
    "                  save_steps=10_000):\n",
    "\n",
    "        train_dataset = TextDataset(\n",
    "            tokenizer=self.tokenizer,\n",
    "            file_path=str(dataset_path),\n",
    "            block_size=256\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer, mlm=False\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(self.data_path / output_name),\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            save_steps=save_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            save_total_limit=2,\n",
    "            logging_dir=str(self.data_path / 'logs')\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        \n",
    "        self.model.save_pretrained(str(self.data_path / output_name))\n",
    "        self.tokenizer.save_pretrained(str(self.data_path / output_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b87c08f-1fbb-4f05-9a54-4fc5d81ca40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, model_name='fine_tuned_model', data_path=DATA_PATH):\n",
    "        model_path = Path(data_path) / model_name\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(str(model_path))\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(str(model_path))\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate_text(self, \n",
    "                    keywords: str,\n",
    "                    max_length=120, \n",
    "                    num_return_sequences=1, \n",
    "                    temperature=1.0, \n",
    "                    top_k=0, \n",
    "                    top_p=1.0, \n",
    "                    do_sample=False):\n",
    "\n",
    "        prompt_text = f\"{keywords} {self.tokenizer.eos_token} \"\n",
    "        \n",
    "        encoded_input = self.tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            encoded_input,\n",
    "            max_length=max_length + len(encoded_input[0]),\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "\n",
    "        for output in outputs:\n",
    "            print(output)\n",
    "        \n",
    "        all_texts = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        \n",
    "        prompt_length = len(self.tokenizer.decode(encoded_input[0], skip_special_tokens=True))\n",
    "        trimmed_texts = [text[prompt_length:] for text in all_texts]\n",
    "        \n",
    "        return {\n",
    "            \"full_texts\": all_texts,\n",
    "            \"generated_texts\": trimmed_texts\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f73edf13-db02-40fb-b630-95763715b0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text       keywords  \\\n",
      "0      Delicate savage. You'll never hold the cinder....         cinder   \n",
      "1      A splash and a cry. Words pulled from the rive...  the riverside   \n",
      "2      Steamy, mist rising. Rocks receiving downward ...    mist rising   \n",
      "3      You were broken glass. But I touched you even ...   broken glass   \n",
      "4      Eyes dance with firelight. The Moon and I are ...     eyes dance   \n",
      "...                                                  ...            ...   \n",
      "14995  So y'all just gonna. Force me to listen to thi...      listen to   \n",
      "14996  Starlin Castro beat. That throw just like Addi...            his   \n",
      "14997  I said the moment. I stopped having fun with i...     the moment   \n",
      "14998  If y'all start acting. Like shit again after t...         ya mom   \n",
      "14999  People worry, too. Much nowadays they forget. ...    they forget   \n",
      "\n",
      "                             input  \\\n",
      "0             cinder <|endoftext|>   \n",
      "1      the riverside <|endoftext|>   \n",
      "2        mist rising <|endoftext|>   \n",
      "3       broken glass <|endoftext|>   \n",
      "4         eyes dance <|endoftext|>   \n",
      "...                            ...   \n",
      "14995      listen to <|endoftext|>   \n",
      "14996            his <|endoftext|>   \n",
      "14997     the moment <|endoftext|>   \n",
      "14998         ya mom <|endoftext|>   \n",
      "14999    they forget <|endoftext|>   \n",
      "\n",
      "                                                  output  \n",
      "0      Delicate savage. You'll never hold the cinder....  \n",
      "1      A splash and a cry. Words pulled from the rive...  \n",
      "2      Steamy, mist rising. Rocks receiving downward ...  \n",
      "3      You were broken glass. But I touched you even ...  \n",
      "4      Eyes dance with firelight. The Moon and I are ...  \n",
      "...                                                  ...  \n",
      "14995  So y'all just gonna. Force me to listen to thi...  \n",
      "14996  Starlin Castro beat. That throw just like Addi...  \n",
      "14997  I said the moment. I stopped having fun with i...  \n",
      "14998  If y'all start acting. Like shit again after t...  \n",
      "14999  People worry, too. Much nowadays they forget. ...  \n",
      "\n",
      "[15000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "finetuner = FineTuner()\n",
    "dataset_path = finetuner.prepare_data(work_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25049d63-171c-44a1-a617-c756aa8c5698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\–ò–ª—å—è\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5872' max='5872' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5872/5872 06:47, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.146200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.791600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.100100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.966600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuner.fine_tune(dataset_path, output_name='fine_tuned_model_gpt_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06d53307-2ad8-4823-9c14-1fa816ede10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cinder',\n",
       " 'the riverside',\n",
       " 'mist rising',\n",
       " 'broken glass',\n",
       " 'eyes dance',\n",
       " 'haiku',\n",
       " 'quit friend',\n",
       " 'wind warms',\n",
       " 'lion limped',\n",
       " 'bloody scalpel']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_keywords = work_data['keywords'].unique().tolist()\n",
    "u_keywords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d69d9f22-d100-4d13-addb-3d325682ff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3099, 28643,   220, 50256,   220, 22503,   416,   317, 23013,    13,\n",
      "          383,  3670,   286,   428,   387, 28643,    13,   632,   338,  1790,\n",
      "           11])\n",
      "tensor([ 3099, 28643,   220, 50256,   220, 22503,   416, 32840,  2584,    84,\n",
      "           13,   383,   938,  6827, 21784,   340,   477,   510,    13,   314,\n",
      "         2630])\n",
      "tensor([ 3099, 28643,   220, 50256,   220, 22503,   416,   317,    13,   311,\n",
      "           13, 32801,    11,   428,   387, 28643,   318,    13, 23762,   290,\n",
      "         1479])\n",
      "Generated Text 1:  Written by Aoi. The title of this haiku. It's short,\n",
      "Generated Text 2:  Written by Ikkyu. The last sentence sums it all up. I wrote\n",
      "Generated Text 3:  Written by A. S. Dawson, this haiku is. Beautiful and free\n",
      "Full Text 1: haiku   Written by Aoi. The title of this haiku. It's short,\n",
      "Full Text 2: haiku   Written by Ikkyu. The last sentence sums it all up. I wrote\n",
      "Full Text 3: haiku   Written by A. S. Dawson, this haiku is. Beautiful and free\n"
     ]
    }
   ],
   "source": [
    "keywords = u_keywords[5]\n",
    "\n",
    "generator = TextGenerator(\n",
    "    model_name='fine_tuned_model_gpt_2',\n",
    "    data_path=DATA_PATH\n",
    ")\n",
    "generated_texts = generator.generate_text(\n",
    "    keywords=keywords,\n",
    "    max_length=16,\n",
    "    num_return_sequences=3,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_k=10,\n",
    "    top_p=0.8\n",
    ")\n",
    "for i, text in enumerate(generated_texts['generated_texts']):\n",
    "    print(f\"Generated Text {i+1}: {text}\")\n",
    "\n",
    "for i, text in enumerate(generated_texts['full_texts']):\n",
    "    print(f\"Full Text {i+1}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caad728-44a1-4623-96ad-d7c50db68f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
